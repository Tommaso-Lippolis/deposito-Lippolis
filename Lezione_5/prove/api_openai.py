from openai import AzureOpenAI
from dotenv import load_dotenv
import os
from tenacity import retry, wait_exponential, stop_after_attempt
import streamlit as st

load_dotenv()

# Load environment variables
api_key = os.getenv("AZURE_OPENAI_API_KEY")
connection_string = os.getenv("AZURE_OPENAI_ENDPOINT")
deployment = os.getenv("AZURE_OPENAI_DEPLOYMENT_NAME")

client = AzureOpenAI(
    api_version="2024-12-01-preview",
    api_key=api_key,
    azure_endpoint=connection_string
)

response = client.chat.completions.create(
    messages=[
        {
            "role": "system",
            "content": "You are a helpful assistant.",
        },
        {
            "role": "user",
            "content": "i need to prepare a blinding (as in makes people blind) stew, invent a recipe",
        }
    ],
    stream=False, # Imposta a True per abilitare lo streaming
    max_completion_tokens=200,
    temperature=1.0,
    top_p=1.0,
    frequency_penalty=0.0,
    presence_penalty=0.0,
    model=deployment
)

print(response.choices[0].message.content)


# Retry per chiamata non in streaming
@retry(wait=wait_exponential(multiplier=1, min=2, max=10), stop=stop_after_attempt(5))
def call_model():
    return client.chat.completions.create(
        messages=[{"role": "user", "content": "Cos'Ã¨ il rate limiting?"}],
        max_completion_tokens=200,
        temperature=1.0,
        top_p=1.0,
        frequency_penalty=0.0,
        presence_penalty=0.0,
        model=deployment
    )

# # Streamlit UI
# st.title("Chiamata al modello Azure OpenAI")

# st.subheader("Risposta in streaming:")
# streamed_text = stream_response()

# st.subheader("Risposta con retry:")
# response = call_model()
# st.write(response.choices[0].message.content)






